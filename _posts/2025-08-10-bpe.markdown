---
layout: post
comments: true
title:  "Byte pair encoding"
excerpt: "simple notes on bpe and implementation"
date:   2025-08-10 11:00:00
mathjax: true
---
<style>
.post-header h1 {
    font-size: 35px;
}
.post pre,
.post code {
    
    font-size: 13px; /* make code smaller for this post... */
}
</style>

This is a very short summary on **Byte-Pair encoding**, focusing on its mechanism rather than its underlying principles. 

The core idea is to iteratively merge frequent text segments and assign them a unique ID. The final dictionary we build is called vocabulary. 

"In the beginning there was the word", just kidding, we have individual letters, numbers, and some "symbols", which totals 256 possible values in 8-bit representation. 

```python
>>> import tiktoken

>>> tokenizer = tiktoken.get_encoding("gpt2")

>>> for i in range(10):
>>>    decode = tokenizer.decode([i])
>>>    print(f"{i} -> {decode}")
```

<img src="{{ site.baseurl }}/assets/bpe_output.png" width="100%" />

BPE then expands upon this initial vocabulary. One might wonder about the limitations of byte array representation: In a byte array, each element or character is mapped to a unique number, meaning N characters will require N distinct numerical representations. However, human language often exhibits frequent patterns, where specific character sequences appear repeatedly. These frequent patterns deserve their own unique IDs. This process effectively reduces the length of the input sequence while gradually increasing the size of the vocabulary.

### BPE algorithm steps:

1. Find frequent pairs
    
    Iterate through the text to find the most frequently occurring pair of characters.

2. Merge and replace

    - Replace the identified pair with a new, unique ID. 
    - Build a corresponding lookup table (vocabulary).

Repeat step 1 and 2, until no new pair can be found. The number of merges is a hyperparameter; for example, for GPT-2, this is 50,257.

